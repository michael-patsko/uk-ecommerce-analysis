{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f9f2f2-42b5-44c0-b5fc-9acfd72b2803",
   "metadata": {},
   "source": [
    "# Introduction to E-Commerce Data Analysis Project\n",
    "This notebook documents my exploration of an e-commerce dataset as part of a self-guided learning project. My goal is to develop and refine my skills in data analysis, focusing on practical application of various tools and techniques. What follows is a comprehensive record of my process, including the challenges I encounter and the insights I gain. The full repository for this project can be found [here](https://github.com/michael-patsko/uk-ecommerce-analysis).\n",
    "\n",
    "## Project Overview\n",
    "The focus of this analysis is the **E-Commerce Analysis - UK** dataset from **Atharva Arya** on Kaggle, found [here](https://www.kaggle.com/datasets/atharvaarya25/e-commerce-analysis-uk/data). This dataset is licensed under the [Community Data License Agreement – Sharing, Version 1.0 (CDLA-Sharing-1.0)](https://cdla.dev/sharing-1-0/) license. More details can be found at the link provided, or in the README of the GitHub repository for this project.\n",
    "\n",
    "Through this project, I aim to enhance my data analysis capabilities and gain hands-on experience with relevant tools. Specifically, I plan to:\n",
    "- Develop proficiency with Python for data analysis\n",
    "- Improve my skills in data cleaning and preprocessing\n",
    "- Explore various data visualisation techniques\n",
    "- Refine my abilities with Jupyter Notebooks, PowerBI, and SQL in the context of data analysis\n",
    "\n",
    "### Tools and Dataset\n",
    "For this analysis, I plan to use:\n",
    "\n",
    "- Python: The primary programming language for data analysis\n",
    "- Pandas: For data manipulation and analysis\n",
    "- Matplotlib and Seaborn: For data visualisation\n",
    "- Jupyter Notebook: The environment for conducting and documenting the analysis\n",
    "- PowerBI: For creating interactive visualisations and dashboards\n",
    "- SQL: For database querying and data manipulation\n",
    "\n",
    "## Analysis\n",
    "With the preliminaries out of the way, I can begin the analysis.\n",
    "\n",
    "First, I begin by installing Pandas and Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f1ebea-5463-42a9-9992-e6a291df826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473db1b-36a5-4217-b2a1-69eef3fdf976",
   "metadata": {},
   "source": [
    "Then, I can import them as `pd` and `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133f7619-f0a9-499b-a493-d27e4ed1603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np # Import both packages under sensibly chosen aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2881058-137a-41c6-a357-e26749e3be7b",
   "metadata": {},
   "source": [
    "When attempting to load the dataset using `pd.read_csv` with default options, I obtained the following **UnicodeDecodeError**:\n",
    "\n",
    "> `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa3 in position 79780: invalid start byte`\n",
    "\n",
    "Looking up the byte 0xa3, I could see that this corresponds to the Unicode character for the pound sign (£), indicating that there may have been an unescaped Unicode character causing the issue. In this case, I could have tried determining the encoding scheme used, or attempted to use a common encoding scheme like ISO-8859-1. Instead, I opted to use the Python codec `unicode_escape` which can gracefully handle these issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67b2a42-0d1c-403b-a3da-5a45d5af17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', encoding='unicode_escape') # Read the CSV data into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea3e6e-e221-43e0-ac44-6abb36fd24ab",
   "metadata": {},
   "source": [
    "This code executes successfully, indicating that this has likely solved the issue.\n",
    "\n",
    "Now, I can print the first 5 rows of our data to get an idea of what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d8107e-1867-4b6d-9e3b-b7277e96fd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1    536365     71053                  WHITE METAL LANTERN         6   \n",
      "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
      "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
      "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "print(df.head()) # Look at the first 5 rows of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcb754-8ef4-4bfb-a437-e5a12db30521",
   "metadata": {},
   "source": [
    "Doing so, we can see we have fields for **InvoiceNo**, **StockCode**, **Description**, **Quantity**, **InvoiceDate**, **UnitPrice**, **CustomerID**, and **Country**. We can see that multiple entries share the same InvoiceNo, suggesting that a single purchase can contain multiple items of varying quantity. We can also use `df.info` and `df.describe` to get some additional details about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc61b308-60ae-497c-aa71-757556d680c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   InvoiceNo    541909 non-null  object \n",
      " 1   StockCode    541909 non-null  object \n",
      " 2   Description  540455 non-null  object \n",
      " 3   Quantity     541909 non-null  int64  \n",
      " 4   InvoiceDate  541909 non-null  object \n",
      " 5   UnitPrice    541909 non-null  float64\n",
      " 6   CustomerID   406829 non-null  float64\n",
      " 7   Country      541909 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n",
      "None             Quantity      UnitPrice     CustomerID\n",
      "count  541909.000000  541909.000000  406829.000000\n",
      "mean        9.552250       4.611114   15287.690570\n",
      "std       218.081158      96.759853    1713.600303\n",
      "min    -80995.000000  -11062.060000   12346.000000\n",
      "25%         1.000000       1.250000   13953.000000\n",
      "50%         3.000000       2.080000   15152.000000\n",
      "75%        10.000000       4.130000   16791.000000\n",
      "max     80995.000000   38970.000000   18287.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.info(),df.describe()) # Get some details about our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d006423-4157-44c5-b2a2-893401be4fc1",
   "metadata": {},
   "source": [
    "From this, we can see that we have a dataset with **541909** entries. It is structured with 8 columns, including two 64-bit floating-point numbers, one 64-bit integer, and five columns categorised as objects, which could represent various data types. We can also see at a glance that about 1500 entries have no value for Description, and about 135,000 entries have no value for CustomerID. We will investigate this in more detail later.\n",
    "\n",
    "Now we know from the data card on Kaggle that this dataset contains erroneous duplicates, and it is our job to remove them. We can first identify duplicates using `df.duplicated`. This returns a [Panda Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) object, containing a list of booleans that tell us whether an entry in our original data frame was a duplicate or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3563590b-c5d5-428f-be2e-da6ed135109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5268\n",
      "0.009721189350979592\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum()) # Return the number of duplicate entries\n",
    "print(df.duplicated().mean()) # Return the percentage of duplicate entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a117775-8043-44d9-a700-00e6beaf8797",
   "metadata": {},
   "source": [
    "From this, we see we have 5268 duplicates, making up just under 1% of our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
